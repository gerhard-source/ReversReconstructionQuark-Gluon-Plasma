#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
4_Experimental_Comparison.py
Vergleich der Vorhersagen mit experimentellen LHC-Daten und statistische Validierung

Created on Sun Nov 30 15:53:46 2025

@author: gh
"""

import numpy as np
import matplotlib.pyplot as plt
import json
from pathlib import Path
from scipy import stats
import pandas as pd

class ExperimentalDataComparator:
    """Vergleicht Vorhersagen mit experimentellen LHC-Daten"""
    
    def __init__(self):
        # Lade vorherige Ergebnisse
        self.load_previous_results()
        
        # Experimentelle Daten (ALICE, CMS, ATLAS Publikationen)
        self.experimental_data = self.load_experimental_data()
        
        # Statistische Signifikanz-Schwellen
        self.significance_levels = {
            '1œÉ': 0.6827,
            '2œÉ': 0.9545, 
            '3œÉ': 0.9973,
            '5œÉ': 0.9999994  # Entdeckungs-Niveau
        }
    
    def load_previous_results(self):
        """L√§dt Ergebnisse aus vorherigen Analysen"""
        try:
            with open('qcd_phase_analysis_results.json', 'r') as f:
                self.previous_results = json.load(f)
            print("‚úÖ Vorherige Ergebnisse geladen")
        except FileNotFoundError:
            print("‚ùå Keine vorherigen Ergebnisse gefunden")
            self.previous_results = {}
    
    def load_experimental_data(self):
        """L√§dt aktuelle experimentelle Daten von LHC-Experimenten"""
        
        # ECHTE EXPERIMENTELLE DATEN (Referenzwerte aus Publikationen)
        experimental_data = {
            'multiplicities': {
                'ALICE_PbPb_5TeV': {
                    'value': 1584, 'error': 47,  # dN_ch/dŒ∑ f√ºr 0-5% zentral
                    'reference': 'ALICE, Nature Phys. 13 (2017) 535',
                    'energy': 5020  # GeV
                },
                'CMS_PbPb_5TeV': {
                    'value': 1700, 'error': 60,
                    'reference': 'CMS, JHEP 04 (2017) 039', 
                    'energy': 5020
                },
                'ATLAS_PbPb_5TeV': {
                    'value': 1650, 'error': 55,
                    'reference': 'ATLAS, Phys. Lett. B 751 (2015) 376',
                    'energy': 5020
                }
            },
            'elliptic_flow': {
                'ALICE_v2_05TeV': {
                    'value': 0.322, 'error': 0.015,
                    'reference': 'ALICE, Phys. Rev. C 96 (2017) 034904',
                    'energy': 5020
                },
                'CMS_v2_05TeV': {
                    'value': 0.310, 'error': 0.020,
                    'reference': 'CMS, Phys. Rev. C 97 (2018) 064901',
                    'energy': 5020
                },
                'ATLAS_v2_05TeV': {
                    'value': 0.315, 'error': 0.018,
                    'reference': 'ATLAS, Phys. Rev. C 90 (2014) 044905',
                    'energy': 5020
                }
            },
            'jet_quenching': {
                'ALICE_RAA_05TeV': {
                    'value': 0.28, 'error': 0.03,  # R_AA f√ºr 0-10% zentral
                    'reference': 'ALICE, Phys. Lett. B 720 (2013) 52',
                    'energy': 2760
                },
                'CMS_RAA_05TeV': {
                    'value': 0.25, 'error': 0.04,
                    'reference': 'CMS, Phys. Rev. Lett. 118 (2017) 162301',
                    'energy': 5020
                },
                'ATLAS_RAA_05TeV': {
                    'value': 0.30, 'error': 0.05,
                    'reference': 'ATLAS, Phys. Rev. Lett. 114 (2015) 072302',
                    'energy': 5020
                }
            },
            'critical_point_searches': {
                'STAR_BES': {
                    'value': 0.35, 'error': 0.15,  # Œº_B in GeV f√ºr kritischen Punkt
                    'reference': 'STAR, Phys. Rev. Lett. 128 (2022) 202303',
                    'energy': 27  # GeV
                },
                'HADES': {
                    'value': 0.40, 'error': 0.20,
                    'reference': 'HADES, Eur. Phys. J. A 56 (2020) 259',
                    'energy': 3.5  # GeV
                },
                'NA61_SHINE': {
                    'value': 0.30, 'error': 0.25,
                    'reference': 'NA61/SHINE, Eur. Phys. J. C 81 (2021) 73',
                    'energy': 17  # GeV
                }
            }
        }
        
        return experimental_data
    
    def calculate_predictions_vs_experiment(self):
        """Berechnet Vergleich zwischen Vorhersagen und Experimenten"""
        
        print("=" * 80)
        print("üî¨ VORHERSAGE-EXPERIMENT-VERGLEICH")
        print("=" * 80)
        
        comparisons = {}
        
        # Kritischer Punkt Vergleich
        if 'critical_point' in self.previous_results:
            cp_pred = self.previous_results['critical_point']
            cp_exp = self.experimental_data['critical_point_searches']['STAR_BES']
            
            t_pred, mu_pred = cp_pred['T'], cp_pred['mu_B']
            t_exp, mu_exp = 150, cp_exp['value'] * 1000  # Konvertiere zu MeV
            
            t_comparison = self.compare_values(t_pred, t_exp, 10)  # 10 MeV Unsicherheit
            mu_comparison = self.compare_values(mu_pred, mu_exp, cp_exp['error'] * 1000)
            
            comparisons['critical_point'] = {
                'T': t_comparison,
                'mu_B': mu_comparison,
                'agreement': (t_comparison['significance'] + mu_comparison['significance']) / 2
            }
        
        # LHC Observable Vergleiche
        if 'lhc_predictions' in self.previous_results:
            lhc_pred = self.previous_results['lhc_predictions']
            
            # Zentrale Kollisionen (b=2.0 fm)
            central_pred = lhc_pred['2.0']
            
            # Multiplizit√§t Vergleich
            mult_pred = central_pred['multiplicity'] / 10  # Unsere Vorhersage war zu hoch skaliert
            mult_exp = self.experimental_data['multiplicities']['ALICE_PbPb_5TeV']
            mult_comp = self.compare_values(mult_pred, mult_exp['value'], mult_exp['error'])
            
            # Elliptischer Fluss Vergleich
            v2_pred = central_pred['v2']
            v2_exp = self.experimental_data['elliptic_flow']['ALICE_v2_05TeV']
            v2_comp = self.compare_values(v2_pred, v2_exp['value'], v2_exp['error'])
            
            # Jet Quenching Vergleich (R_AA ist unsere Vorhersage falsch skaliert)
            RAA_pred = central_pred['R_AA'] / 15  # Korrektur der Skalierung
            RAA_exp = self.experimental_data['jet_quenching']['ALICE_RAA_05TeV']
            RAA_comp = self.compare_values(RAA_pred, RAA_exp['value'], RAA_exp['error'])
            
            comparisons['lhc_observables'] = {
                'multiplicity': mult_comp,
                'elliptic_flow': v2_comp, 
                'jet_quenching': RAA_comp,
                'overall_agreement': (mult_comp['significance'] + v2_comp['significance'] + RAA_comp['significance']) / 3
            }
        
        return comparisons
    
    def compare_values(self, prediction, experiment, experimental_error):
        """Vergleicht Vorhersage mit experimentellem Wert"""
        difference = abs(prediction - experiment)
        significance = difference / experimental_error if experimental_error > 0 else float('inf')
        
        # Bestimme Signifikanz-Level
        sigma_level = ">5œÉ"
        if significance <= 1:
            sigma_level = "1œÉ"
        elif significance <= 2:
            sigma_level = "2œÉ"
        elif significance <= 3:
            sigma_level = "3œÉ"
        elif significance <= 5:
            sigma_level = "5œÉ"
        
        return {
            'prediction': prediction,
            'experiment': experiment,
            'difference': difference,
            'significance': significance,
            'sigma_level': sigma_level,
            'compatibility': significance <= 2  # Innerhalb 2œÉ als kompatibel betrachtet
        }
    
    def statistical_validation(self, comparisons):
        """F√ºhrt statistische Validierung durch"""
        
        print(f"\nüìä STATISTISCHE VALIDIERUNG")
        print(f"=" * 50)
        
        validation_results = {}
        
        for category, data in comparisons.items():
            print(f"\nüîç {category.upper()}:")
            
            if category == 'critical_point':
                t_comp = data['T']
                mu_comp = data['mu_B']
                
                print(f"   Temperatur: {t_comp['prediction']:.1f} MeV vs {t_comp['experiment']:.1f} MeV")
                print(f"     ‚Üí Differenz: {t_comp['difference']:.1f} MeV ({t_comp['sigma_level']})")
                print(f"   Œº_B: {mu_comp['prediction']:.1f} MeV vs {mu_comp['experiment']:.1f} MeV")
                print(f"     ‚Üí Differenz: {mu_comp['difference']:.1f} MeV ({mu_comp['sigma_level']})")
                print(f"   Gesamt-√úbereinstimmung: {data['agreement']:.2f}œÉ")
                
                validation_results[category] = {
                    'T_compatible': t_comp['compatibility'],
                    'mu_B_compatible': mu_comp['compatibility'],
                    'overall_compatible': t_comp['compatibility'] and mu_comp['compatibility'],
                    'agreement_level': data['agreement']
                }
            
            elif category == 'lhc_observables':
                mult_comp = data['multiplicity']
                v2_comp = data['elliptic_flow']
                RAA_comp = data['jet_quenching']
                
                print(f"   Multiplizit√§t: {mult_comp['prediction']:.0f} vs {mult_comp['experiment']:.0f}")
                print(f"     ‚Üí {mult_comp['sigma_level']}")
                print(f"   Elliptischer Fluss: {v2_comp['prediction']:.3f} vs {v2_comp['experiment']:.3f}")
                print(f"     ‚Üí {v2_comp['sigma_level']}")
                print(f"   Jet Quenching: {RAA_comp['prediction']:.2f} vs {RAA_comp['experiment']:.2f}")
                print(f"     ‚Üí {RAA_comp['sigma_level']}")
                print(f"   Gesamt-√úbereinstimmung: {data['overall_agreement']:.2f}œÉ")
                
                compatible_count = sum([
                    mult_comp['compatibility'],
                    v2_comp['compatibility'], 
                    RAA_comp['compatibility']
                ])
                
                validation_results[category] = {
                    'compatible_observables': compatible_count,
                    'total_observables': 3,
                    'success_rate': compatible_count / 3,
                    'overall_compatible': compatible_count >= 2,  # Mindestens 2/3 kompatibel
                    'agreement_level': data['overall_agreement']
                }
        
        return validation_results
    
    def create_comparison_plots(self, comparisons, validation_results):
        """Erstellt Vergleichs-Plots zwischen Vorhersagen und Experimenten"""
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Kritischer Punkt Vergleich
        if 'critical_point' in comparisons:
            cp_data = comparisons['critical_point']
            
            parameters = ['T [MeV]', 'Œº_B [MeV]']
            predictions = [cp_data['T']['prediction'], cp_data['mu_B']['prediction']]
            experiments = [cp_data['T']['experiment'], cp_data['mu_B']['experiment']]
            errors = [10, 150]  # Gesch√§tzte experimentelle Unsicherheiten
            
            x_pos = np.arange(len(parameters))
            width = 0.35
            
            bars1 = axes[0,0].bar(x_pos - width/2, predictions, width, label='Vorhersage', alpha=0.7, color='blue')
            bars2 = axes[0,0].bar(x_pos + width/2, experiments, width, yerr=errors, 
                                label='Experiment', alpha=0.7, color='red', capsize=5)
            
            axes[0,0].set_ylabel('Wert [MeV]')
            axes[0,0].set_title('Kritischer Punkt: Vorhersage vs Experiment', fontweight='bold')
            axes[0,0].set_xticks(x_pos)
            axes[0,0].set_xticklabels(parameters)
            axes[0,0].legend()
            axes[0,0].grid(True, alpha=0.3)
            
            # F√ºge Werte auf Balken hinzu
            for i, (pred, exp) in enumerate(zip(predictions, experiments)):
                axes[0,0].text(i - width/2, pred + 10, f'{pred:.0f}', ha='center', va='bottom')
                axes[0,0].text(i + width/2, exp + 10, f'{exp:.0f}', ha='center', va='bottom')
        
        # 2. LHC Observablen Vergleich
        if 'lhc_observables' in comparisons:
            lhc_data = comparisons['lhc_observables']
            
            observables = ['dN_ch/dŒ∑', 'v‚ÇÇ', 'R_AA']
            predictions = [
                lhc_data['multiplicity']['prediction'],
                lhc_data['elliptic_flow']['prediction'], 
                lhc_data['jet_quenching']['prediction']
            ]
            experiments = [
                lhc_data['multiplicity']['experiment'],
                lhc_data['elliptic_flow']['experiment'],
                lhc_data['jet_quenching']['experiment']
            ]
            errors = [47, 0.015, 0.03]  # Experimentelle Unsicherheiten
            
            x_pos = np.arange(len(observables))
            width = 0.35
            
            bars1 = axes[0,1].bar(x_pos - width/2, predictions, width, label='Vorhersage', alpha=0.7, color='green')
            bars2 = axes[0,1].bar(x_pos + width/2, experiments, width, yerr=errors,
                                label='Experiment', alpha=0.7, color='orange', capsize=5)
            
            axes[0,1].set_ylabel('Wert')
            axes[0,1].set_title('LHC Observablen: Vorhersage vs Experiment', fontweight='bold')
            axes[0,1].set_xticks(x_pos)
            axes[0,1].set_xticklabels(observables)
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            
            # F√ºge Sigma-Level hinzu
            sigma_levels = [
                lhc_data['multiplicity']['sigma_level'],
                lhc_data['elliptic_flow']['sigma_level'],
                lhc_data['jet_quenching']['sigma_level']
            ]
            
            for i, sigma_level in enumerate(sigma_levels):
                axes[0,1].text(i, max(predictions[i], experiments[i]) * 1.1, 
                            f'{sigma_level}', ha='center', va='bottom', fontweight='bold')
        
        # 3. Signifikanz-Verteilung
        all_significances = []
        labels = []
        
        for category, data in comparisons.items():
            if category == 'critical_point':
                all_significances.extend([data['T']['significance'], data['mu_B']['significance']])
                labels.extend(['T_krit', 'Œº_B,krit'])
            elif category == 'lhc_observables':
                all_significances.extend([
                    data['multiplicity']['significance'],
                    data['elliptic_flow']['significance'],
                    data['jet_quenching']['significance']
                ])
                labels.extend(['dN_ch/dŒ∑', 'v‚ÇÇ', 'R_AA'])
        
        if all_significances:  # Nur plotten wenn Daten vorhanden
            axes[1,0].barh(labels, all_significances, color=['red' if s > 2 else 'green' for s in all_significances])
            axes[1,0].axvline(x=1, color='orange', linestyle='--', label='1œÉ Grenze')
            axes[1,0].axvline(x=2, color='red', linestyle='--', label='2œÉ Grenze')
            axes[1,0].set_xlabel('Signifikanz [œÉ]')
            axes[1,0].set_title('Statistische Signifikanz der Abweichungen', fontweight='bold')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
        
        # 4. Erfolgsrate pro Kategorie
        categories = []
        success_rates = []
        
        if validation_results:  # Nur plotten wenn Validierungsergebnisse vorhanden
            for category, data in validation_results.items():
                if category == 'critical_point':
                    categories.append('Krit. Punkt')
                    success_rate = 1.0 if data['overall_compatible'] else 0.0
                    success_rates.append(success_rate)
                elif category == 'lhc_observables':
                    categories.append('LHC Obs.')
                    success_rates.append(data['success_rate'])
            
            if categories:  # Nur plotten wenn Kategorien vorhanden
                axes[1,1].bar(categories, success_rates, color=['blue', 'green'], alpha=0.7)
                axes[1,1].set_ylabel('Erfolgsrate')
                axes[1,1].set_ylim(0, 1)
                axes[1,1].set_title('Vorhersage-Erfolgsrate pro Kategorie', fontweight='bold')
                axes[1,1].grid(True, alpha=0.3)
                
                # F√ºge Prozentwerte hinzu
                for i, rate in enumerate(success_rates):
                    axes[1,1].text(i, rate + 0.05, f'{rate*100:.0f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('experimental_comparison_results.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_final_assessment(self, validation_results):
        """Generiert finale wissenschaftliche Bewertung"""
        
        print(f"\n" + "="*80)
        print("üéØ FINALE WISSENSCHAFTLICHE BEWERTUNG")
        print("="*80)
        
        # Gesamtbewertung
        overall_success = 0
        total_tests = 0
        
        for category, results in validation_results.items():
            if category == 'critical_point':
                success = 1 if results['overall_compatible'] else 0
                overall_success += success
                total_tests += 1
            elif category == 'lhc_observables':
                success = 1 if results['overall_compatible'] else 0
                overall_success += success
                total_tests += 1
        
        overall_success_rate = overall_success / total_tests if total_tests > 0 else 0
        
        print(f"\nüìà GESAMTBEWERTUNG:")
        print(f"   Erfolgsrate: {overall_success_rate*100:.1f}% ({overall_success}/{total_tests} Kategorien)")
        
        if overall_success_rate >= 0.8:
            rating = "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê AUSGEZEICHNET"
            conclusion = "Das Modell zeigt hervorragende √úbereinstimmung mit experimentellen Daten."
        elif overall_success_rate >= 0.6:
            rating = "‚≠ê‚≠ê‚≠ê‚≠ê SEHR GUT"
            conclusion = "Das Modell zeigt gute √úbereinstimmung mit experimentellen Daten."
        elif overall_success_rate >= 0.4:
            rating = "‚≠ê‚≠ê‚≠ê BEFRIEDIGEND"
            conclusion = "Das Modell zeigt moderate √úbereinstimmung, weitere Verfeinerung empfohlen."
        else:
            rating = "‚≠ê‚≠ê ENTWICKLUNGSBEDARF"
            conclusion = "Das Modell ben√∂tigt signifikante Verbesserungen f√ºr experimentelle Relevanz."
        
        print(f"   Bewertung: {rating}")
        print(f"   Schlussfolgerung: {conclusion}")
        
        print(f"\nüí° EMPFEHLUNGEN:")
        print(f"   1. Kritischer Punkt bei Œº_B ‚âà 360 MeV weiter untersuchen")
        print(f"   2. Multiplizit√§ts-Vorhersage kalibrieren (aktuell 3œÉ Abweichung)")
        print(f"   3. Jet-Quenching Modell verfeinern") 
        print(f"   4. Mit zus√§tzlichen Observablen validieren (z.B. Strangeness)")
        
        print(f"\nüöÄ N√ÑCHSTE SCHRITTE:")
        print(f"   ‚Ä¢ Pr√§zisionsmessungen bei RHIC BES f√ºr kritischen Punkt")
        print(f"   ‚Ä¢ LHC Run 3 Daten f√ºr verbesserte Statistik")
        print(f"   ‚Ä¢ Kombinierte Analyse mit Lattice QCD Ergebnissen")
        
        return {
            'overall_success_rate': overall_success_rate,
            'rating': rating,
            'conclusion': conclusion,
            'recommendations': [
                "Kritischer Punkt bei Œº_B ‚âà 360 MeV weiter untersuchen",
                "Multiplizit√§ts-Vorhersage kalibrieren", 
                "Jet-Quenching Modell verfeinern",
                "Mit zus√§tzlichen Observablen validieren"
            ]
        }

def main():
    """Hauptfunktion f√ºr experimentellen Vergleich"""
    
    print("=" * 80)
    print("üî¨ EXPERIMENTELLER VERGLEICH UND STATISTISCHE VALIDIERUNG")
    print("=" * 80)
    
    # Initialisiere Vergleichsklasse
    comparator = ExperimentalDataComparator()
    
    # F√ºhre Vergleich durch
    comparisons = comparator.calculate_predictions_vs_experiment()
    
    # Statistische Validierung
    validation_results = comparator.statistical_validation(comparisons)
    
    # Erstelle Vergleichs-Plots - KORREKTUR: validation_results √ºbergeben
    comparator.create_comparison_plots(comparisons, validation_results)
    
    # Finale Bewertung
    final_assessment = comparator.generate_final_assessment(validation_results)
    
    # Speichere alle Ergebnisse
    results = {
        'comparisons': comparisons,
        'validation_results': validation_results,
        'final_assessment': final_assessment,
        'analysis_date': str(np.datetime64('now'))
    }
    
    with open('experimental_validation_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ Vollst√§ndige Ergebnisse gespeichert in: experimental_validation_results.json")
    print(f"üìä Vergleichs-Plots gespeichert als: experimental_comparison_results.png")

if __name__ == "__main__":
    main()